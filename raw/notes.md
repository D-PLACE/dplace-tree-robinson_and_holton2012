# Notes

Files in ./original/ have been generated by running BEAST1 using the XML provided in the supplementary material following the specifications in the paper. Minor stochastic differences may be present.

Branches are in units of time, but this is meaningless as there were no calibrations entered.

## Data:

> From the 400-item list we removed obvious recent introductions (such as ‘corn’) and
> known loans from non-Alor-Pantar languages (such as proto-Austro- nesian *takaw ‘to
> steal’). We also removed several items for which data were missing for more than half
> of the twelve languages in the sample or which were largely redundant (e.g., we only
> included ‘dolphin’ and not ‘whale’ because the two were the same for most languages).
> Te remaining 351 lexical items were coded numerically for cognacy as described above.
> Crucially, detectable intra- family borrowings were coded as distinct cognate classes
> as described above. In addition to these twelve languages, we also included
> proto-Alor-Pantar as a dis- tinct taxon, coding each of the 97 lexical items in the
> dataset for which pAP forms have been reconstructed. Each lexical item that is a
> regular reflex of a pAP recon- struction was coded as belonging to the same cognate
> class as the pAP reconstruction. Tis process resulted in a 13 × 351 matrix (13 × 351
> = 4,563 character states).




## Methods:

| Model                                | Score    | Program       | Comment            |
|--------------------------------------|----------|---------------|--------------------|
| Neighbornet (Lexicon)                |          | Splitstree    |                    |
| Delta Score (Lexicon)                |          | Splitstree    |                    |
| Neighbornet (Phonology)              |          | Splitstree    |                    |
| Delta Score (Phonology)              |          | Splitstree    |                    |
| Stochastic Dollo + relaxed           |          | BEAST 1.72    | Best fitting model |
| ?                                    |          | MrBayes 3.2.1 |                    |



## Analysis:

> We ran each model for at least 10 million iterations with a sample rate of 1000 and a 
> burn-in of 25 percent. We did four runs on the same dataset for each model, and each
> converged afer approximately 1.5 million iterations. 

